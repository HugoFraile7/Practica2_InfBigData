# ğŸ™ï¸ PrÃ¡ctica Madrid Sostenible - Infraestructura de Almacenamiento para la Ciudad Inteligente

Este repositorio contiene la infraestructura y scripts necesarios para construir un **Data Lakehouse** que integra datos pÃºblicos de movilidad, urbanismo, medioambiente, energÃ­a y participaciÃ³n ciudadana para el anÃ¡lisis y la toma de decisiones sostenibles en Madrid.

## ğŸ“Š Diagrama de Infraestructura

![Arquitectura del Data Lake y Data Warehouse](diagrama_infraestructura.png)

La infraestructura combina un **Data Lake multicapa** y un **Data Warehouse dimensional**, estructurado en zonas:

- **RAW ZONE**: Almacena datos originales.
- **CLEAN ZONE**: Contiene datos transformados y validados.
- **ACCESS ZONE**: Proporciona datos listos para anÃ¡lisis y visualizaciÃ³n.

ğŸ“Œ **Casos de uso**:

- **Pregunta 1**: Cuaderno Jupyter con anÃ¡lisis visual desde ACCESS ZONE.
- **Pregunta 2**: Consultas SQL en PostgreSQL sobre datos limpios.
- **Pregunta 3**: Dashboards en **Apache Superset** para anÃ¡lisis visual ciudadano.

---

## ğŸ§© Modelo de Datos DiseÃ±ado

*(Pendiente de documentaciÃ³n)*

---

## ğŸ”„ Procesos de TransformaciÃ³n ETL (Extract, Transform, Load)

### ğŸŸ  Fase 1: ExtracciÃ³n

Datasets originales:

- `trafico-horario.csv`
- `bicimad-usos.csv`
- `parkings-rotacion.csv` + `ext_aparcamientos_info.csv`
- `dump-bbdd-municipal.sql`
- `avisamadrid.json`

ğŸ“¦ Subidos a **MinIO** (`raw` zone) mediante `upload_file_to_minio`.

ğŸ“„ ConversiÃ³n previa:
- `avisamadrid.json` â†’ CSV con `extract_json_to_dataframe`
- `dump-bbdd-municipal.sql` â†’ varios `.csv` con `extract_sql_to_dataframes`
- FusiÃ³n de aparcamientos con `merge`

---

### ğŸ”µ Fase 2: TransformaciÃ³n

Transformaciones aplicadas:

- EstandarizaciÃ³n de formatos
- EliminaciÃ³n de duplicados
- ConversiÃ³n de fechas y tipos
- Validaciones: no nulos, unicidad, integridad

ğŸ“Œ **Ejemplos por dataset**:

- **TrÃ¡fico**: limpieza de duplicados, tipado de fechas
- **Bicimad**: validaciÃ³n de `usuario_id`
- **Parkings**: conversiÃ³n de coordenadas y tarifas
- **EnergÃ­a**: control de mÃ©tricas energÃ©ticas
- **Edificios/Distritos**: validaciÃ³n de latitudes y antigÃ¼edad
- **Zonas verdes**: normalizaciÃ³n booleana y categÃ³rica
- **Avisamadrid**: integridad mediante claves y fechas

---

### ğŸŸ¢ Fase 3: Carga

*(Detalle en la secciÃ³n de puesta en marcha)*

---

## âš™ï¸ Puesta en Marcha de la Infraestructura

### â–¶ï¸ Arranque con Docker Compose

```bash
docker compose up -d
```

### ğŸ Dockerfile personalizado (python-client)

Incluye:

- pandas, pyarrow, matplotlib
- minio, mysql-connector-python, trino
- great-expectations, etc.

Conectividad con **MinIO**, **Trino**, **MariaDB**, **PostgreSQL**.

---

## ğŸª£ Buckets de MinIO

- `raw-ingestion-zone`
- `clean-zone`
- `process-zone`
- `access-zone`
- `govern-zone-metadata`

---

## ğŸš¦ Flujo por Zonas

### ğŸ” Zona 1: Raw

Carga de datos originales:

```bash
docker exec -it python-client python /scripts/01_ingest_data.py
```

### ğŸ§¹ Zona 2: Clean

Limpieza y validaciÃ³n:

```bash
docker exec -it python-client python /scripts/02_clean_data.py
```

### ğŸ”§ Zona 3: Process

Agregaciones, KPIs, transformaciones para anÃ¡lisis:

```bash
docker exec -it python-client python /scripts/03_access_zone.py
```

---

## ğŸ““ AnÃ¡lisis Visual (Pregunta 1)

Cuaderno Jupyter: `notebooks/01_congestion_vehiculos.ipynb`

- Carga desde: `trafico_congestion_por_hora.parquet`
- Herramientas: pandas, matplotlib
- Solo anÃ¡lisis, sin transformaciÃ³n

---

## ğŸ¢ Data Warehouse (PostgreSQL)

### 4ï¸âƒ£ Crear modelo en PostgreSQL

```bash
docker exec -it python-client python /scripts/04_create_datawarehouse.py
```

### 5ï¸âƒ£ Cargar datos limpios

```bash
docker exec -it python-client python /scripts/05_load_warehouse_data.py
```

### â“ Preguntas de Negocio (Task 2)

**6ï¸âƒ£ Rutas BiciMAD mÃ¡s populares**

```bash
docker exec -it python-client python /scripts/06_query_bicimad_routes.py
```

**7ï¸âƒ£ Densidad vs Transporte**

```bash
docker exec -it python-client python /scripts/07_query_demografia_transporte.py
```

---

## ğŸ“ Estructura del Proyecto

```
/scripts
â”œâ”€â”€ 01_ingest_data.py
â”œâ”€â”€ 02_clean_data.py
â”œâ”€â”€ 03_access_zone.py
â”œâ”€â”€ 04_create_datawarehouse.py
â”œâ”€â”€ 05_load_warehouse_data.py
â”œâ”€â”€ 06_query_bicimad_routes.py
â”œâ”€â”€ 07_query_demografia_transporte.py

/notebooks
â””â”€â”€ 01_congestion_vehiculos.ipynb
```

---

## âœ… ComprobaciÃ³n Final

Verifica servicios activos:

```bash
docker compose ps
```

---

Â¡Listo para trabajar con tu pipeline de datos sostenible en Madrid! ğŸš€
