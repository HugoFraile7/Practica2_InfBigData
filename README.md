# Pr√°ctica Madrid Sostenible - Infraestructura de Almacenamiento para la Ciudad Inteligente

Este repositorio contiene la infraestructura y scripts necesarios para construir un **Data Lakehouse** que integra datos p√∫blicos de movilidad, urbanismo, medioambiente, energ√≠a y participaci√≥n ciudadana para el an√°lisis y la toma de decisiones sostenibles en Madrid.

## Diagrama de Infraestructura

![Arquitectura del Data Lake y Data Warehouse](diagrama_infraestructura.png)


La infraestructura dise√±ada combina un **Data Lake multicapa** y un **Data Warehouse dimensional** para garantizar una gesti√≥n flexible, escalable y orientada al an√°lisis de datos urbanos. En la **RAW ZONE** del Data Lake se almacenan los datos en su formato original, mientras que en la **CLEAN ZONE** se aplican transformaciones y validaciones para asegurar calidad y coherencia. Finalmente, la **ACCESS ZONE** ofrece datos listos para el an√°lisis, que alimentan tanto notebooks como bases de datos anal√≠ticas.

- üîç **Pregunta 1:** Se abordar√° mediante un cuaderno `.ipynb`, trabajando directamente sobre los datasets ya transformados en la *ACCESS ZONE*.
- üóÉÔ∏è **Pregunta 2:** Se resolver√° con consultas **SQL** sobre una base de datos **PostgreSQL** que contiene las tablas generadas a partir de los datos limpios.
- üìä **Pregunta 3:** Se responder√° mediante la construcci√≥n de **dashboards en Apache Superset**, conectados al **Data Warehouse**, con el objetivo de facilitar el an√°lisis visual a perfiles no t√©cnicos como ciudadanos o asociaciones vecinales.

---

##  Modelo de Datos Dise√±ado

*(Pendiente de documentaci√≥n)*

---

##  Procesos de Transformaci√≥n ETL (Extract, Transform, Load)

###  Fase 1: Extracci√≥n

Datasets originales:

- `trafico-horario.csv`
- `bicimad-usos.csv`
- `parkings-rotacion.csv` + `ext_aparcamientos_info.csv`
- `dump-bbdd-municipal.sql`
- `avisamadrid.json`

 Suidos a **MinIO** (`raw` zone) mediante `upload_file_to_minio`.

Conversi√≥n previa:
- `avisamadrid.json` ‚Üí CSV con `extract_json_to_dataframe`
- `dump-bbdd-municipal.sql` ‚Üí varios `.csv` con `extract_sql_to_dataframes`
- Fusi√≥n de aparcamientos con `merge`

---

### Fase 2: Transformaci√≥n

Transformaciones aplicadas:

- Estandarizaci√≥n de formatos
- Eliminaci√≥n de duplicados
- Conversi√≥n de fechas y tipos
- Validaciones: no nulos, unicidad, integridad

 **Ejemplos por dataset**:

- **Tr√°fico**: limpieza de duplicados, tipado de fechas
- **Bicimad**: validaci√≥n de `usuario_id`
- **Parkings**: conversi√≥n de coordenadas y tarifas
- **Energ√≠a**: control de m√©tricas energ√©ticas
- **Edificios/Distritos**: validaci√≥n de latitudes y antig√ºedad
- **Zonas verdes**: normalizaci√≥n booleana y categ√≥rica
- **Avisamadrid**: integridad mediante claves y fechas

---

### Fase 3: Carga

1. **Clonar el repositorio**  
   ```bash
   git clone https://github.com/HugoFraile7/Practica2_InfBigData
   cd proyecto-madridsostenible


---

## Puesta en Marcha de la Infraestructura

### Arranque con Docker Compose

```bash
docker compose up -d 
```

Crea en docker todos los contenedores necesarios para el desarrollo de la pr√°ctica(Conectividad con **MinIO**, **Trino**, **MariaDB**, **PostgreSQL**) adem√°s desde un **Dcokerfile** auxiliar instala todas las dependecias necearias.



- `raw-ingestion-zone`
- `clean-zone`
- `process-zone`
- `access-zone`
- `govern-zone-metadata(informaci√≥n de realizaci√≥n de la pr√°ctica)`

---

## Flujo por Zonas

### Zona 1: Raw

Carga de datos originales:

```bash
docker exec -it python-client python /scripts/01_ingest_data.py
```

### Zona 2: Clean

Limpieza y normalizaci√≥n de los datos:

```bash
docker exec -it python-client python /scripts/02_clean_data.py
```

### Zona 3: Process

Agregaciones, KPIs, transformaciones para an√°lisis:

```bash
docker exec -it python-client python /scripts/03_access_zone.py
```

---

## An√°lisis Visual (Pregunta 1)

Cuaderno Jupyter: `notebooks/01_congestion_vehiculos.ipynb`

-En este cuaderno se carga el dataset **trafico.parquet**, y se realizan las visualizaciones pertinentes para responder a la primera cuesti√≥n.

---

## Data Warehouse (PostgreSQL)

### 4Ô∏è Crear modelo en PostgreSQL
-Se crean las tablas necesarias.
```bash
docker exec -it python-client python /scripts/04_create_datawarehouse.py
```

### 5Ô∏è Carga datos
-Se insertan los datos en las tablas previamente creadas en la etapa anterior.
```bash
docker exec -it python-client python /scripts/05_load_warehouse_data.py
```

### Preguntas de Negocio (Task 2)

** 6Ô∏è Rutas BiciMAD m√°s populares**

```bash
docker exec -it python-client python /scripts/06_query_bicimad_routes.py
```

** Densidad vs Transporte**

```bash
docker exec -it python-client python /scripts/07_query_demografia_transporte.py
```

---

##  Estructura del Proyecto

```
/scripts
‚îú‚îÄ‚îÄ 01_ingest_data.py
‚îú‚îÄ‚îÄ 02_clean_data.py
‚îú‚îÄ‚îÄ 03_access_zone.py
‚îú‚îÄ‚îÄ 04_create_datawarehouse.py
‚îú‚îÄ‚îÄ 05_load_warehouse_data.py
‚îú‚îÄ‚îÄ 06_query_bicimad_routes.py
‚îú‚îÄ‚îÄ 07_query_demografia_transporte.py

/notebooks
‚îî‚îÄ‚îÄ 01_congestion_vehiculos.ipynb
üöÄ
