# Pr谩ctica Madrid Sostenible - Infraestructura de Almacenamiento para la Ciudad Inteligente


Este repositorio contiene la infraestructura y scripts necesarios para construir un **Data Lakehouse** que integra datos p煤blicos de movilidad, urbanismo, medioambiente, energ铆a y participaci贸n ciudadana para el an谩lisis y la toma de decisiones sostenibles en Madrid. 

## Diagrama de infraestructura

![Arquitectura del Data Lake y Data Warehouse](diagrama_infraestructura.png)



La infraestructura dise帽ada combina un **Data Lakehouse** que integra un **Data Lake multicapa** y un **Data Warehouse dimensional** para garantizar una gesti贸n flexible, escalable y orientada al an谩lisis de datos urbanos. En la **RAW ZONE** del Data Lake se almacenan los datos en su formato original, mientras que en la **CLEAN ZONE** se aplican transformaciones y validaciones para asegurar calidad y coherencia. Finalmente, la **ACCESS ZONE** ofrece datos listos para el an谩lisis, que alimentan tanto notebooks como bases de datos anal铆ticas.

- **Pregunta 1:** Se abordar谩 mediante un cuaderno `.ipynb`, trabajando directamente sobre los datasets ya transformados en la *ACCESS ZONE*.
- **Pregunta 2:** Se resolver谩 con consultas **SQL** sobre una base de datos **PostgreSQL** que contiene las tablas generadas a partir de los datos limpios.
- **Pregunta 3:** Se responder谩 mediante la construcci贸n de **dashboards en Apache Superset**, conectados al **Data Warehouse**, con el objetivo de facilitar el an谩lisis visual a perfiles no t茅cnicos como ciudadanos o asociaciones vecinales.


## Modelo de datos dise帽ado


## З Procesos de Transformaci贸n ETL (Extract, Transform, Load)

### Fase 1: Extracci贸n (Extract)

Contamos con 6 datasets: **trafico-horario.csv** (volumen y tipo de veh铆culos por hora), **bicimad-usos.csv** (trayectos y tipo de usuario), **parkings-rotacion.csv** (ocupaci贸n y ubicaci贸n de aparcamientos), **ext_aparcamientos_info.csv** (una extensi贸n del dataset anterior), **dump-bbdd-municipal.sql** (una base de datos SQL con m煤ltiples tablas) y **avisamadrid.json** (avisos ciudadanos geolocalizados).

Los datos se cargan inicialmente en un bucket de MinIO, dentro de la zona **raw**, utilizando la funci贸n `upload_file_to_minio` definida en `utils.py`. Esta zona conserva los archivos originales tal como fueron extra铆dos, creando una carpeta por dataset.

Antes de limpiar los datos, se realiza una conversi贸n del archivo JSON a CSV mediante `extract_json_to_dataframe`, que convierte el archivo `avisamadrid.json` a un DataFrame. Asimismo, se utiliza `extract_sql_to_dataframes` para cargar las tablas definidas en el script SQL original, generando los datasets `distritos.csv`, `edificios_publicos.csv`, `estaciones_transporte.csv`, `lineas_transporte.csv` y `zonas_verdes.csv`. Tambi茅n se realiza la fusi贸n de `ext_aparcamientos_info.csv` y `parkings-rotacion.csv` mediante una operaci贸n `merge`.

Posteriormente, se aplican transformaciones espec铆ficas a cada DataFrame y se almacenan como archivos `.parquet` en la **clean zone**, estructurados por tem谩ticas. Estas transformaciones est谩n implementadas en funciones espec铆ficas para cada dataset, que realizan los siguientes cambios:

---

### Fase 2: Transformaci贸n (Transform)

Siguiendo los principios de transici贸n de la zona RAW a CLEAN, se aplicaron transformaciones centradas en:

- Estandarizaci贸n de formatos
- Eliminaci贸n de duplicados
- Conversi贸n de fechas y tipos num茅ricos
- Aplicaci贸n de reglas de validaci贸n (no nulos, unicidad, coherencia)

#### Ejemplos espec铆ficos por dataset:

- **Tr谩fico**: conversi贸n de `fecha_hora` a datetime y eliminaci贸n de duplicados.
- **Bicimad**: tipado de fechas y validaci贸n de columnas clave como `usuario_id`.
- **Parkings**: uni贸n de CSVs, conversi贸n de coordenadas y tarifas, y deduplicaci贸n por clave compuesta.
- **Consumo energ茅tico**: conversi贸n de fechas y validaci贸n de m茅tricas como `consumo_electrico_kwh`.
- **Distritos y edificios**: validaci贸n de latitudes, a帽os de construcci贸n y relaciones con otras entidades.
- **Zonas verdes**: transformaci贸n de booleanos, normalizaci贸n de tipos y control de valores clave.
- **Avisamadrid**: fechas de reporte/resoluci贸n convertidas y control de integridad mediante claves primarias.

Estas transformaciones responden a las tareas comunes descritas en el marco ETL: limpieza, validaci贸n, control de nulos y tipado, tal y como se propone en la documentaci贸n acad茅mica del proyecto (PDF de referencia).

---

### Fase 3: Carga (Load)
